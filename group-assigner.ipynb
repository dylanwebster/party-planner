{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the stuff we need\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Google Sheet and store it in a dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scope\n",
    "scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "\n",
    "# Get the path to your credentials file from the environment variable\n",
    "creds_path = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')\n",
    "\n",
    "# Add your service account credentials file\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(creds_path, scope)\n",
    "\n",
    "# Authorize the client\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# Define the Google Sheet URL\n",
    "sheet_url = os.getenv('GOOGLE_SHEET_URL')\n",
    "\n",
    "# Extract the spreadsheet ID from the URL\n",
    "spreadsheet_id = sheet_url.split('/d/')[1].split('/')[0]\n",
    "\n",
    "# Open the Google Sheet by ID\n",
    "spreadsheet = client.open_by_key(spreadsheet_id)\n",
    "sheet = client.open_by_key(spreadsheet_id).sheet1  # or use .get_worksheet(index) for specific sheet\n",
    "\n",
    "# Get all values in the sheet\n",
    "data = sheet.get_all_records()\n",
    "\n",
    "# Convert the data to a Pandas DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the functions that will do the work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_balanced_groups(df, num_groups=None, employees_per_group=None, separate_remote=False):\n",
    "    if num_groups is None and employees_per_group is None:\n",
    "        raise ValueError(\"Either num_groups or employees_per_group must be specified.\")\n",
    "    \n",
    "    # Filter out employees who are not attending\n",
    "    df = df[df['Attending'] == 'Yes']\n",
    "\n",
    "    # Determine the number of groups\n",
    "    if num_groups is None:\n",
    "        num_groups = int(np.ceil(len(df) / employees_per_group))\n",
    "    \n",
    "    # Calculate base group size and number of larger groups\n",
    "    base_size = len(df) // num_groups\n",
    "    larger_groups_count = len(df) % num_groups\n",
    "    \n",
    "    # Separate remote and non-remote if required\n",
    "    if separate_remote:\n",
    "        remote_df = df[df['Remote'] == 'Yes']\n",
    "        non_remote_df = df[df['Remote'] == 'No']\n",
    "        \n",
    "        remote_groups = _create_balanced_groups(remote_df, num_groups, base_size, larger_groups_count)\n",
    "        non_remote_groups = _create_balanced_groups(non_remote_df, num_groups, base_size, larger_groups_count)\n",
    "        \n",
    "        # Combine remote and non-remote groups\n",
    "        groups = [r + nr for r, nr in zip(remote_groups, non_remote_groups)]\n",
    "    else:\n",
    "        groups = _create_balanced_groups(df, num_groups, base_size, larger_groups_count)\n",
    "    \n",
    "    return groups\n",
    "\n",
    "def _create_balanced_groups(df, num_groups, base_size, larger_groups_count):\n",
    "    # Shuffle the DataFrame to ensure randomness\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Group by team\n",
    "    team_groups = defaultdict(list)\n",
    "    for _, row in df.iterrows():\n",
    "        team_groups[row['Team']].append(row)\n",
    "    \n",
    "    # Create empty groups\n",
    "    groups = [[] for _ in range(num_groups)]\n",
    "    \n",
    "    # Distribute team members evenly across groups\n",
    "    for team, members in team_groups.items():\n",
    "        for i, member in enumerate(members):\n",
    "            target_group = i % num_groups\n",
    "            if len(groups[target_group]) < base_size or (target_group < larger_groups_count and len(groups[target_group]) == base_size):\n",
    "                groups[target_group].append(member)\n",
    "            else:\n",
    "                # Find the next available group\n",
    "                for j in range(num_groups):\n",
    "                    if len(groups[j]) < base_size or (j < larger_groups_count and len(groups[j]) == base_size):\n",
    "                        groups[j].append(member)\n",
    "                        break\n",
    "    \n",
    "    return groups\n",
    "\n",
    "def groups_to_dataframe(groups):\n",
    "    # Find the maximum group size\n",
    "    max_group_size = max(len(group) for group in groups)\n",
    "    \n",
    "    # Create a dictionary to hold group data\n",
    "    group_data = {f'Group {i+1}': [None] * max_group_size for i in range(len(groups))}\n",
    "    \n",
    "    # Fill the dictionary with group members\n",
    "    for i, group in enumerate(groups):\n",
    "        for j, member in enumerate(group):\n",
    "            group_data[f'Group {i+1}'][j] = f\"{member['Emails']} ({member['Team']}, {member['Remote']})\"\n",
    "    \n",
    "    # Convert the dictionary to a DataFrame\n",
    "    return pd.DataFrame(group_data)\n",
    "\n",
    "def write_to_google_sheet(df, spreadsheet, sheet_name='Groups'):\n",
    "    try:\n",
    "        worksheet = spreadsheet.add_worksheet(title=sheet_name, rows=str(len(df)+1), cols=str(len(df.columns)))\n",
    "    except gspread.exceptions.APIError:\n",
    "        worksheet = spreadsheet.worksheet(sheet_name)\n",
    "        worksheet.clear()\n",
    "    \n",
    "    worksheet.update([df.columns.values.tolist()] + df.values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = split_into_balanced_groups(df, num_groups=4, separate_remote=False)\n",
    "group_df = groups_to_dataframe(groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the groups to the Google Sheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_google_sheet(group_df, spreadsheet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowflake-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
